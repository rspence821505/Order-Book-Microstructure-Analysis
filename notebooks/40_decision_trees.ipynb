{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Decision Trees - AAPL Microstructure Prediction\n",
    "\n",
    "**Objective:** Train Decision Tree classifiers with hyperparameter tuning and compare against baseline logistic regression.\n",
    "\n",
    "**Inputs:**\n",
    "- Features with regimes from notebook 30 (`AAPL_features_with_regimes.parquet`)\n",
    "- Baseline results from notebook 35\n",
    "\n",
    "**Models:**\n",
    "- Decision Tree with Gini impurity\n",
    "- Decision Tree with Entropy criterion\n",
    "- Hyperparameter tuning: max_depth, min_samples_split, min_samples_leaf\n",
    "\n",
    "**Outputs:**\n",
    "- Tuned Decision Tree models\n",
    "- Performance comparison vs. baseline\n",
    "- Decision path visualizations\n",
    "- Feature importance analysis\n",
    "- Depth saturation analysis\n",
    "\n",
    "**Target Variable:** Next-period mid-price direction (up=1, down=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Import project config\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.config import (\n",
    "    PROCESSED_DATA_DIR,\n",
    "    MODELS_DIR,\n",
    "    FIGURES_DIR,\n",
    "    TABLES_DIR\n",
    ")\n",
    "\n",
    "# Create output directories\n",
    "dt_dir = FIGURES_DIR / 'decision_trees'\n",
    "dt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Setup complete\")\n",
    "print(f\"  Data directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"  Models directory: {MODELS_DIR}\")\n",
    "print(f\"  Figures directory: {dt_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features with regimes from notebook 30\n",
    "print(\"Loading features with regimes...\")\n",
    "features_with_regimes = pd.read_parquet(\n",
    "    PROCESSED_DATA_DIR / 'AAPL_features_with_regimes.parquet'\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Features with regimes: {features_with_regimes.shape}\")\n",
    "print(f\"  Date range: {features_with_regimes['timestamp'].min()} to {features_with_regimes['timestamp'].max()}\")\n",
    "print(f\"  Columns: {len(features_with_regimes.columns)}\")\n",
    "\n",
    "# Load baseline results for comparison\n",
    "with open(MODELS_DIR / 'baseline_results.json', 'r') as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "print(\"\\n✓ Loaded baseline results\")\n",
    "print(f\"  Baseline L2 test accuracy: {baseline_results['lr_l2']['test_accuracy']:.4f}\")\n",
    "print(f\"  Baseline L2 test F1: {baseline_results['lr_l2']['test_f1']:.4f}\")\n",
    "print(f\"  Baseline L2 test ROC-AUC: {baseline_results['lr_l2']['test_roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Create Target Variable and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find price column\n",
    "price_candidates = ['agg_mid_price', 'agg_close', 'mid_price', 'close']\n",
    "price_col = next((col for col in price_candidates if col in features_with_regimes.columns), None)\n",
    "\n",
    "if price_col is None:\n",
    "    raise ValueError(f\"No price column found. Available columns: {features_with_regimes.columns.tolist()[:20]}\")\n",
    "\n",
    "print(f\"Using price column: {price_col}\")\n",
    "\n",
    "# Create target: 1 if next price > current price, 0 otherwise\n",
    "features_with_regimes['price_next'] = features_with_regimes[price_col].shift(-1)\n",
    "features_with_regimes['target'] = (features_with_regimes['price_next'] > features_with_regimes[price_col]).astype(int)\n",
    "\n",
    "# Remove last row (no future price)\n",
    "features_with_regimes = features_with_regimes[:-1].copy()\n",
    "\n",
    "print(f\"\\n✓ Target variable created\")\n",
    "print(f\"  Shape after removing last row: {features_with_regimes.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "target_counts = features_with_regimes['target'].value_counts()\n",
    "print(f\"  Down (0): {target_counts.get(0, 0):,} ({100*target_counts.get(0, 0)/len(features_with_regimes):.1f}%)\")\n",
    "print(f\"  Up (1): {target_counts.get(1, 0):,} ({100*target_counts.get(1, 0)/len(features_with_regimes):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Feature Selection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to exclude from features\n",
    "exclude_cols = [\n",
    "    'timestamp', 'target', 'price_next',\n",
    "    'regime_hmm', 'regime_hawkes', 'regime_hawkes_binary', 'hmm_state',\n",
    "    'time_hour', 'time_minute', 'time_minutes_since_open', 'time_session',\n",
    "    'hour', 'minute', 'time_of_day'\n",
    "]\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in features_with_regimes.columns \n",
    "                if col not in exclude_cols]\n",
    "\n",
    "# Select only numeric features\n",
    "X = features_with_regimes[feature_cols].select_dtypes(include=[np.number])\n",
    "feature_cols = X.columns.tolist()\n",
    "\n",
    "print(f\"Selected {len(feature_cols)} numeric features\")\n",
    "print(f\"\\nFirst 20 features:\")\n",
    "print(feature_cols[:20])\n",
    "\n",
    "# Get target\n",
    "y = features_with_regimes['target'].values\n",
    "\n",
    "# Handle missing and infinite values\n",
    "X = X.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "print(f\"\\n✓ Feature matrix: {X.shape}\")\n",
    "print(f\"✓ Target vector: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split (Time-Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series split (no shuffling)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y[:split_idx]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]:,} samples ({100*len(X_train)/len(X):.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({100*len(X_test)/len(X):.1f}%)\")\n",
    "\n",
    "print(\"\\nTrain target distribution:\")\n",
    "train_counts = pd.Series(y_train).value_counts()\n",
    "print(f\"  Down (0): {train_counts.get(0, 0):,} ({100*train_counts.get(0, 0)/len(y_train):.1f}%)\")\n",
    "print(f\"  Up (1): {train_counts.get(1, 0):,} ({100*train_counts.get(1, 0)/len(y_train):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Baseline Decision Tree (Untuned)\n",
    "\n",
    "Train a simple decision tree with default parameters to establish baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline decision tree (default parameters)\n",
    "print(\"Training baseline Decision Tree (default parameters)...\")\n",
    "\n",
    "dt_baseline = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "dt_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_base = dt_baseline.predict(X_train)\n",
    "y_test_pred_base = dt_baseline.predict(X_test)\n",
    "y_test_proba_base = dt_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE DECISION TREE (DEFAULT PARAMETERS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTree depth: {dt_baseline.get_depth()}\")\n",
    "print(f\"Number of leaves: {dt_baseline.get_n_leaves()}\")\n",
    "\n",
    "print(\"\\nTRAIN SET:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_train, y_train_pred_base):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_train, y_train_pred_base):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_train, y_train_pred_base):.4f}\")\n",
    "print(f\"  F1-score:  {f1_score(y_train, y_train_pred_base):.4f}\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_test_pred_base):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_base):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_base):.4f}\")\n",
    "print(f\"  F1-score:  {f1_score(y_test, y_test_pred_base):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_proba_base):.4f}\")\n",
    "\n",
    "# Train-test gap (overfitting indicator)\n",
    "train_test_gap = accuracy_score(y_train, y_train_pred_base) - accuracy_score(y_test, y_test_pred_base)\n",
    "print(f\"\\nTrain-Test Gap: {train_test_gap:.4f} ({100*train_test_gap:.1f}% overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. Depth Saturation Analysis\n",
    "\n",
    "Test different max_depth values to find optimal depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different depths\n",
    "print(\"Testing different max_depth values...\")\n",
    "\n",
    "depths = [3, 4, 5, 6, 7, 8, 10, 12, 15, 20, None]\n",
    "depth_results = []\n",
    "\n",
    "for depth in depths:\n",
    "    dt = DecisionTreeClassifier(\n",
    "        max_depth=depth,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = dt.predict(X_train)\n",
    "    y_test_pred = dt.predict(X_test)\n",
    "    y_test_proba = dt.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    depth_results.append({\n",
    "        'max_depth': depth if depth is not None else 'None',\n",
    "        'actual_depth': dt.get_depth(),\n",
    "        'n_leaves': dt.get_n_leaves(),\n",
    "        'train_acc': accuracy_score(y_train, y_train_pred),\n",
    "        'test_acc': accuracy_score(y_test, y_test_pred),\n",
    "        'test_f1': f1_score(y_test, y_test_pred),\n",
    "        'test_roc_auc': roc_auc_score(y_test, y_test_proba),\n",
    "        'overfit_gap': accuracy_score(y_train, y_train_pred) - accuracy_score(y_test, y_test_pred)\n",
    "    })\n",
    "\n",
    "depth_df = pd.DataFrame(depth_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEPTH SATURATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(depth_df.to_string(index=False))\n",
    "\n",
    "# Find optimal depth\n",
    "optimal_idx = depth_df['test_f1'].idxmax()\n",
    "optimal_depth = depth_df.loc[optimal_idx, 'max_depth']\n",
    "print(f\"\\n✓ Optimal max_depth: {optimal_depth}\")\n",
    "print(f\"  Test F1-score: {depth_df.loc[optimal_idx, 'test_f1']:.4f}\")\n",
    "print(f\"  Test accuracy: {depth_df.loc[optimal_idx, 'test_acc']:.4f}\")\n",
    "print(f\"  Overfitting gap: {depth_df.loc[optimal_idx, 'overfit_gap']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8. Visualize Depth Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot depth analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Convert None to large number for plotting\n",
    "plot_depths = [d if d != 'None' else 25 for d in depth_df['max_depth']]\n",
    "\n",
    "# Plot 1: Accuracy vs Depth\n",
    "axes[0, 0].plot(plot_depths, depth_df['train_acc'], 'o-', linewidth=2, markersize=8, label='Train')\n",
    "axes[0, 0].plot(plot_depths, depth_df['test_acc'], 's-', linewidth=2, markersize=8, label='Test')\n",
    "axes[0, 0].axhline(y=baseline_results['lr_l2']['test_accuracy'], \n",
    "                    color='red', linestyle='--', linewidth=2, label='Baseline (LR L2)')\n",
    "axes[0, 0].set_xlabel('Max Depth', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 0].set_title('Accuracy vs Max Depth', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "axes[0, 0].set_xticks(plot_depths)\n",
    "axes[0, 0].set_xticklabels([str(d) if d != 'None' else 'None' for d in depth_df['max_depth']], rotation=45)\n",
    "\n",
    "# Plot 2: F1-score vs Depth\n",
    "axes[0, 1].plot(plot_depths, depth_df['test_f1'], 'o-', linewidth=2, markersize=8, color='green')\n",
    "axes[0, 1].axhline(y=baseline_results['lr_l2']['test_f1'], \n",
    "                    color='red', linestyle='--', linewidth=2, label='Baseline (LR L2)')\n",
    "axes[0, 1].set_xlabel('Max Depth', fontsize=11)\n",
    "axes[0, 1].set_ylabel('F1-Score', fontsize=11)\n",
    "axes[0, 1].set_title('F1-Score vs Max Depth', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xticks(plot_depths)\n",
    "axes[0, 1].set_xticklabels([str(d) if d != 'None' else 'None' for d in depth_df['max_depth']], rotation=45)\n",
    "\n",
    "# Plot 3: ROC-AUC vs Depth\n",
    "axes[1, 0].plot(plot_depths, depth_df['test_roc_auc'], 'o-', linewidth=2, markersize=8, color='purple')\n",
    "axes[1, 0].axhline(y=baseline_results['lr_l2']['test_roc_auc'], \n",
    "                    color='red', linestyle='--', linewidth=2, label='Baseline (LR L2)')\n",
    "axes[1, 0].set_xlabel('Max Depth', fontsize=11)\n",
    "axes[1, 0].set_ylabel('ROC-AUC', fontsize=11)\n",
    "axes[1, 0].set_title('ROC-AUC vs Max Depth', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].set_xticks(plot_depths)\n",
    "axes[1, 0].set_xticklabels([str(d) if d != 'None' else 'None' for d in depth_df['max_depth']], rotation=45)\n",
    "\n",
    "# Plot 4: Overfitting Gap vs Depth\n",
    "axes[1, 1].plot(plot_depths, depth_df['overfit_gap'], 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1, 1].set_xlabel('Max Depth', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Overfitting Gap (Train - Test)', fontsize=11)\n",
    "axes[1, 1].set_title('Overfitting vs Max Depth', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].set_xticks(plot_depths)\n",
    "axes[1, 1].set_xticklabels([str(d) if d != 'None' else 'None' for d in depth_df['max_depth']], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(dt_dir / 'depth_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to {dt_dir / 'depth_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning with Grid Search\n",
    "\n",
    "Tune max_depth, min_samples_split, and min_samples_leaf using time-series cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [5, 6, 7, 8, 10],\n",
    "    'min_samples_split': [20, 50, 100, 200],\n",
    "    'min_samples_leaf': [10, 20, 50],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter tuning with GridSearchCV...\")\n",
    "print(f\"Parameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "\n",
    "# Time-series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest cross-validation F1-score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 10. Train Final Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model from grid search\n",
    "dt_tuned = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_tuned = dt_tuned.predict(X_train)\n",
    "y_test_pred_tuned = dt_tuned.predict(X_test)\n",
    "y_test_proba_tuned = dt_tuned.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"=\"*80)\n",
    "print(\"TUNED DECISION TREE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTree structure:\")\n",
    "print(f\"  Max depth: {dt_tuned.get_depth()}\")\n",
    "print(f\"  Number of leaves: {dt_tuned.get_n_leaves()}\")\n",
    "print(f\"  Min samples split: {dt_tuned.min_samples_split}\")\n",
    "print(f\"  Min samples leaf: {dt_tuned.min_samples_leaf}\")\n",
    "print(f\"  Criterion: {dt_tuned.criterion}\")\n",
    "\n",
    "print(\"\\nTRAIN SET:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_train, y_train_pred_tuned):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_train, y_train_pred_tuned):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_train, y_train_pred_tuned):.4f}\")\n",
    "print(f\"  F1-score:  {f1_score(y_train, y_train_pred_tuned):.4f}\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
    "test_f1_tuned = f1_score(y_test, y_test_pred_tuned)\n",
    "test_auc_tuned = roc_auc_score(y_test, y_test_proba_tuned)\n",
    "\n",
    "print(f\"  Accuracy:  {test_acc_tuned:.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_tuned):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_tuned):.4f}\")\n",
    "print(f\"  F1-score:  {test_f1_tuned:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_auc_tuned:.4f}\")\n",
    "\n",
    "# Compare to baseline\n",
    "print(\"\\nIMPROVEMENT OVER BASELINE (Logistic Regression L2):\")\n",
    "acc_improvement = test_acc_tuned - baseline_results['lr_l2']['test_accuracy']\n",
    "f1_improvement = test_f1_tuned - baseline_results['lr_l2']['test_f1']\n",
    "auc_improvement = test_auc_tuned - baseline_results['lr_l2']['test_roc_auc']\n",
    "\n",
    "print(f\"  Accuracy:  {acc_improvement:+.4f} ({100*acc_improvement/baseline_results['lr_l2']['test_accuracy']:+.1f}%)\")\n",
    "print(f\"  F1-score:  {f1_improvement:+.4f} ({100*f1_improvement/baseline_results['lr_l2']['test_f1']:+.1f}%)\")\n",
    "print(f\"  ROC-AUC:   {auc_improvement:+.4f} ({100*auc_improvement/baseline_results['lr_l2']['test_roc_auc']:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': dt_tuned.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Filter to non-zero importance\n",
    "feature_importance_nonzero = feature_importance[feature_importance['importance'] > 0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE (TOP 20)\")\n",
    "print(\"=\"*80)\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\nFeatures used: {len(feature_importance_nonzero)}/{len(feature_cols)}\")\n",
    "print(f\"Top 10 features account for {100*feature_importance.head(10)['importance'].sum():.1f}% of total importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_features = feature_importance.head(20)\n",
    "ax.barh(range(len(top_features)), top_features['importance'].values)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'].values)\n",
    "ax.set_xlabel('Importance (Gini/Entropy)', fontsize=11)\n",
    "ax.set_title('Top 20 Feature Importances - Tuned Decision Tree', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(dt_dir / 'feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to {dt_dir / 'feature_importance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 12. Visualize Decision Tree\n",
    "\n",
    "Visualize a simplified version of the tree (limited depth for readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a smaller tree for visualization (max_depth=4)\n",
    "dt_viz = DecisionTreeClassifier(\n",
    "    max_depth=4,\n",
    "    min_samples_split=100,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "dt_viz.fit(X_train, y_train)\n",
    "\n",
    "# Visualize tree\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "plot_tree(\n",
    "    dt_viz,\n",
    "    feature_names=feature_cols,\n",
    "    class_names=['Down', 'Up'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=8,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Decision Tree Visualization (depth=4 for readability)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(dt_dir / 'tree_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to {dt_dir / 'tree_visualization.png'}\")\n",
    "print(f\"\\nNote: Simplified tree (depth=4) shown for readability.\")\n",
    "print(f\"Actual tuned tree has depth={dt_tuned.get_depth()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 13. Extract Decision Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export tree as text rules\n",
    "tree_rules = export_text(dt_viz, feature_names=feature_cols, max_depth=3)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DECISION RULES (simplified tree, depth=4, showing first 3 levels)\")\n",
    "print(\"=\"*80)\n",
    "print(tree_rules)\n",
    "\n",
    "# Save rules to file\n",
    "with open(dt_dir / 'decision_rules.txt', 'w') as f:\n",
    "    f.write(tree_rules)\n",
    "\n",
    "print(f\"\\n✓ Full rules saved to {dt_dir / 'decision_rules.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 14. Confusion Matrix and ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred_tuned)\n",
    "im = axes[0].imshow(cm, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title('Confusion Matrix - Tuned Decision Tree', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted', fontsize=11)\n",
    "axes[0].set_ylabel('Actual', fontsize=11)\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(['Down', 'Up'])\n",
    "axes[0].set_yticklabels(['Down', 'Up'])\n",
    "\n",
    "# Annotate cells\n",
    "for row in range(2):\n",
    "    for col in range(2):\n",
    "        axes[0].text(col, row, str(cm[row, col]), \n",
    "                    ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba_tuned)\n",
    "axes[1].plot(fpr, tpr, linewidth=2, label=f'Decision Tree (AUC={test_auc_tuned:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=11)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize=11)\n",
    "axes[1].set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='lower right', fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(dt_dir / 'performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to {dt_dir / 'performance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 15. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "with open(MODELS_DIR / 'decision_tree_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(dt_tuned, f)\n",
    "with open(MODELS_DIR / 'decision_tree_baseline.pkl', 'wb') as f:\n",
    "    pickle.dump(dt_baseline, f)\n",
    "\n",
    "print(\"✓ Models saved:\")\n",
    "print(f\"  {MODELS_DIR / 'decision_tree_tuned.pkl'}\")\n",
    "print(f\"  {MODELS_DIR / 'decision_tree_baseline.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    'dt_baseline': {\n",
    "        'train_accuracy': float(accuracy_score(y_train, y_train_pred_base)),\n",
    "        'test_accuracy': float(accuracy_score(y_test, y_test_pred_base)),\n",
    "        'test_precision': float(precision_score(y_test, y_test_pred_base)),\n",
    "        'test_recall': float(recall_score(y_test, y_test_pred_base)),\n",
    "        'test_f1': float(f1_score(y_test, y_test_pred_base)),\n",
    "        'test_roc_auc': float(roc_auc_score(y_test, y_test_proba_base)),\n",
    "        'tree_depth': int(dt_baseline.get_depth()),\n",
    "        'n_leaves': int(dt_baseline.get_n_leaves())\n",
    "    },\n",
    "    'dt_tuned': {\n",
    "        'train_accuracy': float(accuracy_score(y_train, y_train_pred_tuned)),\n",
    "        'test_accuracy': float(test_acc_tuned),\n",
    "        'test_precision': float(precision_score(y_test, y_test_pred_tuned)),\n",
    "        'test_recall': float(recall_score(y_test, y_test_pred_tuned)),\n",
    "        'test_f1': float(test_f1_tuned),\n",
    "        'test_roc_auc': float(test_auc_tuned),\n",
    "        'tree_depth': int(dt_tuned.get_depth()),\n",
    "        'n_leaves': int(dt_tuned.get_n_leaves()),\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_f1_score': float(grid_search.best_score_)\n",
    "    },\n",
    "    'improvement_over_baseline': {\n",
    "        'accuracy': float(acc_improvement),\n",
    "        'f1': float(f1_improvement),\n",
    "        'roc_auc': float(auc_improvement),\n",
    "        'accuracy_pct': float(100*acc_improvement/baseline_results['lr_l2']['test_accuracy']),\n",
    "        'f1_pct': float(100*f1_improvement/baseline_results['lr_l2']['test_f1']),\n",
    "        'roc_auc_pct': float(100*auc_improvement/baseline_results['lr_l2']['test_roc_auc'])\n",
    "    },\n",
    "    'depth_analysis': depth_df.to_dict('records')\n",
    "}\n",
    "\n",
    "with open(MODELS_DIR / 'decision_tree_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {MODELS_DIR / 'decision_tree_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature importance\n",
    "feature_importance.to_csv(TABLES_DIR / 'dt_feature_importance.csv', index=False)\n",
    "depth_df.to_csv(TABLES_DIR / 'dt_depth_analysis.csv', index=False)\n",
    "\n",
    "print(\"✓ Tables saved:\")\n",
    "print(f\"  {TABLES_DIR / 'dt_feature_importance.csv'}\")\n",
    "print(f\"  {TABLES_DIR / 'dt_depth_analysis.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## 16. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DECISION TREE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Train samples: {len(X_train):,}\")\n",
    "print(f\"  Test samples: {len(X_test):,}\")\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\nBaseline Decision Tree (default parameters):\")\n",
    "print(f\"  Test Accuracy: {results['dt_baseline']['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1-score: {results['dt_baseline']['test_f1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC:  {results['dt_baseline']['test_roc_auc']:.4f}\")\n",
    "print(f\"  Tree depth: {results['dt_baseline']['tree_depth']}\")\n",
    "print(f\"  Number of leaves: {results['dt_baseline']['n_leaves']}\")\n",
    "\n",
    "print(f\"\\nTuned Decision Tree:\")\n",
    "print(f\"  Test Accuracy: {results['dt_tuned']['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1-score: {results['dt_tuned']['test_f1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC:  {results['dt_tuned']['test_roc_auc']:.4f}\")\n",
    "print(f\"  Tree depth: {results['dt_tuned']['tree_depth']}\")\n",
    "print(f\"  Number of leaves: {results['dt_tuned']['n_leaves']}\")\n",
    "\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for param, value in results['dt_tuned']['best_params'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nImprovement over Baseline (Logistic Regression L2):\")\n",
    "print(f\"  Accuracy:  {results['improvement_over_baseline']['accuracy']:+.4f} ({results['improvement_over_baseline']['accuracy_pct']:+.1f}%)\")\n",
    "print(f\"  F1-score:  {results['improvement_over_baseline']['f1']:+.4f} ({results['improvement_over_baseline']['f1_pct']:+.1f}%)\")\n",
    "print(f\"  ROC-AUC:   {results['improvement_over_baseline']['roc_auc']:+.4f} ({results['improvement_over_baseline']['roc_auc_pct']:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  - Decision Tree {'outperforms' if test_acc_tuned > baseline_results['lr_l2']['test_accuracy'] else 'underperforms'} logistic regression baseline\")\n",
    "print(f\"  - Optimal depth found at {optimal_depth} (performance saturates beyond this)\")\n",
    "print(f\"  - {len(feature_importance_nonzero)} features used out of {len(feature_cols)} available\")\n",
    "print(f\"  - Top 10 features account for {100*feature_importance.head(10)['importance'].sum():.1f}% of importance\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  - Notebook 45: Random Forest (ensemble of trees)\")\n",
    "print(f\"  - Notebook 50: Gradient Boosting (sequential boosting)\")\n",
    "print(f\"  - Compare ensemble methods vs. single tree performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
