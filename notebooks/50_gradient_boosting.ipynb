{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Gradient Boosting - Microstructure Prediction\n",
    "\n",
    "**Objective:** Train Gradient Boosting classifiers and compare sequential boosting vs. bagging (Random Forest).\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "- Features with regimes from notebook 30 (`AAPL_features_with_regimes.parquet`)\n",
    "- Baseline results from notebook 35\n",
    "- Decision Tree results from notebook 40\n",
    "- Random Forest results from notebook 45\n",
    "\n",
    "**Models:**\n",
    "\n",
    "- Gradient Boosting Classifier (standard GBM)\n",
    "- HistGradientBoosting Classifier (optimized for large datasets)\n",
    "- Hyperparameter tuning: learning_rate, max_depth, n_estimators, early stopping\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "- Tuned Gradient Boosting models\n",
    "- Performance comparison vs. Random Forest, Decision Tree, and baseline\n",
    "- Feature importance analysis\n",
    "- Learning curves and early stopping analysis\n",
    "- Boosting vs. bagging comparison\n",
    "\n",
    "**Target Variable:** Next-period mid-price direction (up=1, down=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    log_loss,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "plt.style.use(\"seaborn-v0_8-paper\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.weight\": \"bold\",\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"axes.titleweight\": \"bold\",\n",
    "        \"axes.linewidth\": 1.2,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Import project config\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.config import PROCESSED_DATA_DIR, MODELS_DIR, FIGURES_DIR, TABLES_DIR\n",
    "\n",
    "# Create output directories\n",
    "gb_dir = FIGURES_DIR / \"gradient_boosting\"\n",
    "gb_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Setup complete\")\n",
    "print(f\"  Data directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"  Models directory: {MODELS_DIR}\")\n",
    "print(f\"  Figures directory: {gb_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Load Data and Previous Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features with regimes from notebook 30\n",
    "print(\"Loading features with regimes...\")\n",
    "features_with_regimes = pd.read_parquet(\n",
    "    PROCESSED_DATA_DIR / \"AAPL_features_with_regimes.parquet\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Features with regimes: {features_with_regimes.shape}\")\n",
    "\n",
    "# Load previous results for comparison\n",
    "with open(MODELS_DIR / \"baseline_results.json\", \"r\") as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "with open(MODELS_DIR / \"decision_tree_results.json\", \"r\") as f:\n",
    "    dt_results = json.load(f)\n",
    "\n",
    "with open(MODELS_DIR / \"random_forest_results.json\", \"r\") as f:\n",
    "    rf_results = json.load(f)\n",
    "\n",
    "print(\"\\n✓ Loaded previous results\")\n",
    "print(f\"\\nBaseline (Logistic Regression L2):\")\n",
    "print(f\"  Test Accuracy: {baseline_results['lr_l2']['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1:       {baseline_results['lr_l2']['test_f1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC:  {baseline_results['lr_l2']['test_roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nDecision Tree (Tuned):\")\n",
    "print(f\"  Test Accuracy: {dt_results['dt_tuned']['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1:       {dt_results['dt_tuned']['test_f1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC:  {dt_results['dt_tuned']['test_roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nRandom Forest (Tuned):\")\n",
    "print(f\"  Test Accuracy: {rf_results['rf_tuned']['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1:       {rf_results['rf_tuned']['test_f1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC:  {rf_results['rf_tuned']['test_roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Create Target Variable and Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find price column\n",
    "price_candidates = [\"agg_mid_price\", \"agg_close\", \"mid_price\", \"close\"]\n",
    "price_col = next(\n",
    "    (col for col in price_candidates if col in features_with_regimes.columns), None\n",
    ")\n",
    "\n",
    "if price_col is None:\n",
    "    raise ValueError(\n",
    "        f\"No price column found. Available columns: {features_with_regimes.columns.tolist()[:20]}\"\n",
    "    )\n",
    "\n",
    "print(f\"Using price column: {price_col}\")\n",
    "\n",
    "# Create target: 1 if next price > current price, 0 otherwise\n",
    "features_with_regimes[\"price_next\"] = features_with_regimes[price_col].shift(-1)\n",
    "features_with_regimes[\"target\"] = (\n",
    "    features_with_regimes[\"price_next\"] > features_with_regimes[price_col]\n",
    ").astype(int)\n",
    "\n",
    "# Remove last row (no future price)\n",
    "features_with_regimes = features_with_regimes[:-1].copy()\n",
    "\n",
    "print(f\"\\n✓ Target variable created\")\n",
    "print(f\"  Shape: {features_with_regimes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Feature Selection and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to exclude from features\n",
    "exclude_cols = [\n",
    "    \"timestamp\",\n",
    "    \"target\",\n",
    "    \"price_next\",\n",
    "    \"regime_hmm\",\n",
    "    \"regime_hawkes\",\n",
    "    \"regime_hawkes_binary\",\n",
    "    \"hmm_state\",\n",
    "    \"time_hour\",\n",
    "    \"time_minute\",\n",
    "    \"time_minutes_since_open\",\n",
    "    \"time_session\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"time_of_day\",\n",
    "]\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in features_with_regimes.columns if col not in exclude_cols]\n",
    "\n",
    "# Select only numeric features\n",
    "X = features_with_regimes[feature_cols].select_dtypes(include=[np.number])\n",
    "feature_cols = X.columns.tolist()\n",
    "\n",
    "# Get target\n",
    "y = features_with_regimes[\"target\"].values\n",
    "\n",
    "# Handle missing and infinite values\n",
    "X = X.fillna(method=\"ffill\").fillna(method=\"bfill\").fillna(0)\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "print(f\"✓ Feature matrix: {X.shape}\")\n",
    "print(f\"✓ Target vector: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split (Time-Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series split (no shuffling)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y[:split_idx]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]:,} samples ({100*len(X_train)/len(X):.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({100*len(X_test)/len(X):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Baseline Gradient Boosting\n",
    "\n",
    "Train with default parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline gradient boosting\n",
    "print(\"Training baseline Gradient Boosting (100 estimators, default parameters)...\")\n",
    "\n",
    "gb_baseline = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "gb_baseline.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_base = gb_baseline.predict(X_train)\n",
    "y_test_pred_base = gb_baseline.predict(X_test)\n",
    "y_test_proba_base = gb_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE GRADIENT BOOSTING (100 estimators, default parameters)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "print(f\"Number of estimators: {gb_baseline.n_estimators}\")\n",
    "print(f\"Learning rate: {gb_baseline.learning_rate}\")\n",
    "print(f\"Max depth: {gb_baseline.max_depth}\")\n",
    "\n",
    "print(\"\\nTRAIN SET:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_train, y_train_pred_base):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_train, y_train_pred_base):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_train, y_train_pred_base):.4f}\")\n",
    "print(f\"  F1-score:  {f1_score(y_train, y_train_pred_base):.4f}\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_test_pred_base):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_base):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_base):.4f}\")\n",
    "print(f\"  F1-score:  {f1_score(y_test, y_test_pred_base):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_proba_base):.4f}\")\n",
    "\n",
    "# Compare to Random Forest\n",
    "print(\"\\nCOMPARISON TO RANDOM FOREST:\")\n",
    "rf_test_acc = rf_results[\"rf_tuned\"][\"test_accuracy\"]\n",
    "rf_test_f1 = rf_results[\"rf_tuned\"][\"test_f1\"]\n",
    "gb_test_acc = accuracy_score(y_test, y_test_pred_base)\n",
    "gb_test_f1 = f1_score(y_test, y_test_pred_base)\n",
    "\n",
    "print(\n",
    "    f\"  Accuracy:  {gb_test_acc - rf_test_acc:+.4f} ({100*(gb_test_acc - rf_test_acc)/rf_test_acc:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1-score:  {gb_test_f1 - rf_test_f1:+.4f} ({100*(gb_test_f1 - rf_test_f1)/rf_test_f1:+.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. Learning Rate Analysis\n",
    "\n",
    "Test different learning rates to understand bias-variance tradeoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "print(\"Testing different learning rates...\")\n",
    "\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"  Training with learning_rate={lr}...\", end=\" \")\n",
    "\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=lr, max_depth=5, random_state=42\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    gb.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = gb.predict(X_train)\n",
    "    y_test_pred = gb.predict(X_test)\n",
    "    y_test_proba = gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    lr_results.append(\n",
    "        {\n",
    "            \"learning_rate\": lr,\n",
    "            \"train_time\": train_time,\n",
    "            \"train_acc\": accuracy_score(y_train, y_train_pred),\n",
    "            \"test_acc\": accuracy_score(y_test, y_test_pred),\n",
    "            \"test_f1\": f1_score(y_test, y_test_pred),\n",
    "            \"test_roc_auc\": roc_auc_score(y_test, y_test_proba),\n",
    "            \"overfit_gap\": accuracy_score(y_train, y_train_pred)\n",
    "            - accuracy_score(y_test, y_test_pred),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"done ({train_time:.1f}s)\")\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LEARNING RATE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(lr_df.to_string(index=False))\n",
    "\n",
    "# Find optimal learning rate\n",
    "optimal_idx = lr_df[\"test_f1\"].idxmax()\n",
    "optimal_lr = lr_df.loc[optimal_idx, \"learning_rate\"]\n",
    "print(f\"\\n✓ Optimal learning_rate: {optimal_lr}\")\n",
    "print(f\"  Test F1-score: {lr_df.loc[optimal_idx, 'test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8. Visualize Learning Rate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning rate analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Accuracy vs Learning Rate\n",
    "axes[0].plot(\n",
    "    lr_df[\"learning_rate\"],\n",
    "    lr_df[\"train_acc\"],\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    label=\"Train\",\n",
    ")\n",
    "axes[0].plot(\n",
    "    lr_df[\"learning_rate\"],\n",
    "    lr_df[\"test_acc\"],\n",
    "    \"s-\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    label=\"Test\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Learning Rate\", fontsize=11)\n",
    "axes[0].set_ylabel(\"Accuracy\", fontsize=11)\n",
    "axes[0].set_title(\"Accuracy vs Learning Rate\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xscale(\"log\")\n",
    "\n",
    "# Plot 2: F1-score vs Learning Rate\n",
    "axes[1].plot(\n",
    "    lr_df[\"learning_rate\"],\n",
    "    lr_df[\"test_f1\"],\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    color=\"green\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Learning Rate\", fontsize=11)\n",
    "axes[1].set_ylabel(\"F1-Score\", fontsize=11)\n",
    "axes[1].set_title(\"F1-Score vs Learning Rate\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xscale(\"log\")\n",
    "\n",
    "# Plot 3: Overfitting Gap vs Learning Rate\n",
    "axes[2].plot(\n",
    "    lr_df[\"learning_rate\"],\n",
    "    lr_df[\"overfit_gap\"],\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    color=\"orange\",\n",
    ")\n",
    "axes[2].axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "axes[2].set_xlabel(\"Learning Rate\", fontsize=11)\n",
    "axes[2].set_ylabel(\"Overfitting Gap (Train - Test)\", fontsize=11)\n",
    "axes[2].set_title(\"Overfitting vs Learning Rate\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_xscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(gb_dir / \"learning_rate_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to {gb_dir / 'learning_rate_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning with Randomized Search\n",
    "\n",
    "Tune learning_rate, max_depth, n_estimators, min_samples_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [100, 150, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
    "    \"min_samples_split\": [20, 50, 100],\n",
    "    \"min_samples_leaf\": [10, 20, 50],\n",
    "    \"subsample\": [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter tuning with RandomizedSearchCV...\")\n",
    "print(f\"Parameter distributions:\")\n",
    "for param, values in param_distributions.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# Time-series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Randomized search (sample 50 combinations)\n",
    "random_search = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=tscv,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"\\nTesting {random_search.n_iter} parameter combinations...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANDOMIZED SEARCH RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest cross-validation F1-score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 10. Train Final Tuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model from randomized search\n",
    "gb_tuned = random_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_tuned = gb_tuned.predict(X_train)\n",
    "y_test_pred_tuned = gb_tuned.predict(X_test)\n",
    "y_test_proba_tuned = gb_tuned.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"=\" * 80)\n",
    "print(\"TUNED GRADIENT BOOSTING PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Number of estimators: {gb_tuned.n_estimators}\")\n",
    "print(f\"  Learning rate: {gb_tuned.learning_rate}\")\n",
    "print(f\"  Max depth: {gb_tuned.max_depth}\")\n",
    "print(f\"  Min samples split: {gb_tuned.min_samples_split}\")\n",
    "print(f\"  Min samples leaf: {gb_tuned.min_samples_leaf}\")\n",
    "print(f\"  Subsample: {gb_tuned.subsample}\")\n",
    "\n",
    "print(\"\\nTRAIN SET:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_train, y_train_pred_tuned):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_train, y_train_pred_tuned):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_train, y_train_pred_tuned):.4f}\")\n",
    "print(f\"  F1-score:  {f1_score(y_train, y_train_pred_tuned):.4f}\")\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
    "test_f1_tuned = f1_score(y_test, y_test_pred_tuned)\n",
    "test_auc_tuned = roc_auc_score(y_test, y_test_proba_tuned)\n",
    "\n",
    "print(f\"  Accuracy:  {test_acc_tuned:.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_tuned):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_tuned):.4f}\")\n",
    "print(f\"  F1-score:  {test_f1_tuned:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_auc_tuned:.4f}\")\n",
    "\n",
    "# Overfitting analysis\n",
    "train_test_gap = accuracy_score(y_train, y_train_pred_tuned) - test_acc_tuned\n",
    "print(f\"\\nTrain-Test Gap: {train_test_gap:.4f} ({100*train_test_gap:.1f}%)\")\n",
    "\n",
    "# Compare to all previous models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPROVEMENT OVER BASELINE (Logistic Regression L2):\")\n",
    "print(\"=\" * 80)\n",
    "acc_improvement_base = test_acc_tuned - baseline_results[\"lr_l2\"][\"test_accuracy\"]\n",
    "f1_improvement_base = test_f1_tuned - baseline_results[\"lr_l2\"][\"test_f1\"]\n",
    "auc_improvement_base = test_auc_tuned - baseline_results[\"lr_l2\"][\"test_roc_auc\"]\n",
    "\n",
    "print(\n",
    "    f\"  Accuracy:  {acc_improvement_base:+.4f} ({100*acc_improvement_base/baseline_results['lr_l2']['test_accuracy']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1-score:  {f1_improvement_base:+.4f} ({100*f1_improvement_base/baseline_results['lr_l2']['test_f1']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  ROC-AUC:   {auc_improvement_base:+.4f} ({100*auc_improvement_base/baseline_results['lr_l2']['test_roc_auc']:+.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPROVEMENT OVER DECISION TREE (Tuned):\")\n",
    "print(\"=\" * 80)\n",
    "acc_improvement_dt = test_acc_tuned - dt_results[\"dt_tuned\"][\"test_accuracy\"]\n",
    "f1_improvement_dt = test_f1_tuned - dt_results[\"dt_tuned\"][\"test_f1\"]\n",
    "auc_improvement_dt = test_auc_tuned - dt_results[\"dt_tuned\"][\"test_roc_auc\"]\n",
    "\n",
    "print(\n",
    "    f\"  Accuracy:  {acc_improvement_dt:+.4f} ({100*acc_improvement_dt/dt_results['dt_tuned']['test_accuracy']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1-score:  {f1_improvement_dt:+.4f} ({100*f1_improvement_dt/dt_results['dt_tuned']['test_f1']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  ROC-AUC:   {auc_improvement_dt:+.4f} ({100*auc_improvement_dt/dt_results['dt_tuned']['test_roc_auc']:+.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON TO RANDOM FOREST (Tuned):\")\n",
    "print(\"=\" * 80)\n",
    "acc_improvement_rf = test_acc_tuned - rf_results[\"rf_tuned\"][\"test_accuracy\"]\n",
    "f1_improvement_rf = test_f1_tuned - rf_results[\"rf_tuned\"][\"test_f1\"]\n",
    "auc_improvement_rf = test_auc_tuned - rf_results[\"rf_tuned\"][\"test_roc_auc\"]\n",
    "\n",
    "print(\n",
    "    f\"  Accuracy:  {acc_improvement_rf:+.4f} ({100*acc_improvement_rf/rf_results['rf_tuned']['test_accuracy']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1-score:  {f1_improvement_rf:+.4f} ({100*f1_improvement_rf/rf_results['rf_tuned']['test_f1']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  ROC-AUC:   {auc_improvement_rf:+.4f} ({100*auc_improvement_rf/rf_results['rf_tuned']['test_roc_auc']:+.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BOOSTING vs. BAGGING COMPARISON:\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"Gradient Boosting (sequential): F1={test_f1_tuned:.4f}, ROC-AUC={test_auc_tuned:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Random Forest (parallel):       F1={rf_results['rf_tuned']['test_f1']:.4f}, ROC-AUC={rf_results['rf_tuned']['test_roc_auc']:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\n{'Boosting' if test_f1_tuned > rf_results['rf_tuned']['test_f1'] else 'Bagging'} performs better for this task\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame(\n",
    "    {\"feature\": feature_cols, \"importance\": gb_tuned.feature_importances_}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Filter to non-zero importance\n",
    "feature_importance_nonzero = feature_importance[feature_importance[\"importance\"] > 0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE (TOP 20)\")\n",
    "print(\"=\" * 80)\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\nFeatures used: {len(feature_importance_nonzero)}/{len(feature_cols)}\")\n",
    "print(\n",
    "    f\"Top 10 features account for {100*feature_importance.head(10)['importance'].sum():.1f}% of total importance\"\n",
    ")\n",
    "print(\n",
    "    f\"Top 20 features account for {100*feature_importance.head(20)['importance'].sum():.1f}% of total importance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_features = feature_importance.head(20)\n",
    "ax.barh(range(len(top_features)), top_features[\"importance\"].values)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features[\"feature\"].values)\n",
    "ax.set_xlabel(\"Importance (Loss Reduction)\", fontsize=11)\n",
    "ax.set_title(\n",
    "    \"Top 20 Feature Importances - Tuned Gradient Boosting\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(gb_dir / \"feature_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to {gb_dir / 'feature_importance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 12. Learning Curves - Staged Prediction\n",
    "\n",
    "Analyze how performance evolves with each boosting iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate staged predictions (performance at each boosting iteration)\n",
    "print(\"Computing learning curves (staged predictions)...\")\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for i, (y_train_pred_staged, y_test_pred_staged) in enumerate(\n",
    "    zip(gb_tuned.staged_predict(X_train), gb_tuned.staged_predict(X_test))\n",
    "):\n",
    "    train_scores.append(accuracy_score(y_train, y_train_pred_staged))\n",
    "    test_scores.append(accuracy_score(y_test, y_test_pred_staged))\n",
    "\n",
    "# Create learning curves DataFrame\n",
    "learning_curves = pd.DataFrame(\n",
    "    {\n",
    "        \"iteration\": range(1, len(train_scores) + 1),\n",
    "        \"train_acc\": train_scores,\n",
    "        \"test_acc\": test_scores,\n",
    "        \"overfit_gap\": np.array(train_scores) - np.array(test_scores),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Computed learning curves for {len(train_scores)} iterations\")\n",
    "print(f\"\\nPerformance at key iterations:\")\n",
    "for n in [10, 50, 100, len(train_scores)]:\n",
    "    if n <= len(train_scores):\n",
    "        idx = n - 1\n",
    "        print(\n",
    "            f\"  Iteration {n:3d}: Train={train_scores[idx]:.4f}, Test={test_scores[idx]:.4f}, Gap={train_scores[idx]-test_scores[idx]:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Accuracy over iterations\n",
    "axes[0].plot(\n",
    "    learning_curves[\"iteration\"],\n",
    "    learning_curves[\"train_acc\"],\n",
    "    linewidth=2,\n",
    "    label=\"Train\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "axes[0].plot(\n",
    "    learning_curves[\"iteration\"],\n",
    "    learning_curves[\"test_acc\"],\n",
    "    linewidth=2,\n",
    "    label=\"Test\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "axes[0].set_xlabel(\"Boosting Iteration\", fontsize=11)\n",
    "axes[0].set_ylabel(\"Accuracy\", fontsize=11)\n",
    "axes[0].set_title(\n",
    "    \"Learning Curves - Accuracy over Iterations\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Overfitting gap over iterations\n",
    "axes[1].plot(\n",
    "    learning_curves[\"iteration\"],\n",
    "    learning_curves[\"overfit_gap\"],\n",
    "    linewidth=2,\n",
    "    color=\"orange\",\n",
    ")\n",
    "axes[1].axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "axes[1].set_xlabel(\"Boosting Iteration\", fontsize=11)\n",
    "axes[1].set_ylabel(\"Overfitting Gap (Train - Test)\", fontsize=11)\n",
    "axes[1].set_title(\n",
    "    \"Overfitting Evolution over Iterations\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(gb_dir / \"learning_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to {gb_dir / 'learning_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 13. Confusion Matrix and ROC Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred_tuned)\n",
    "im = axes[0].imshow(cm, cmap=\"Blues\", aspect=\"auto\")\n",
    "axes[0].set_title(\n",
    "    \"Confusion Matrix - Tuned Gradient Boosting\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].set_xlabel(\"Predicted\", fontsize=11)\n",
    "axes[0].set_ylabel(\"Actual\", fontsize=11)\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels([\"Down\", \"Up\"])\n",
    "axes[0].set_yticklabels([\"Down\", \"Up\"])\n",
    "\n",
    "# Annotate cells\n",
    "for row in range(2):\n",
    "    for col in range(2):\n",
    "        axes[0].text(\n",
    "            col,\n",
    "            row,\n",
    "            str(cm[row, col]),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "# ROC curve - compare all models\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_test_proba_tuned)\n",
    "axes[1].plot(\n",
    "    fpr_gb,\n",
    "    tpr_gb,\n",
    "    linewidth=2.5,\n",
    "    label=f\"Gradient Boosting (AUC={test_auc_tuned:.3f})\",\n",
    "    color=\"purple\",\n",
    ")\n",
    "axes[1].axhline(\n",
    "    y=rf_results[\"rf_tuned\"][\"test_roc_auc\"],\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Random Forest (AUC={rf_results['rf_tuned']['test_roc_auc']:.3f})\",\n",
    ")\n",
    "axes[1].axhline(\n",
    "    y=dt_results[\"dt_tuned\"][\"test_roc_auc\"],\n",
    "    color=\"blue\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Decision Tree (AUC={dt_results['dt_tuned']['test_roc_auc']:.3f})\",\n",
    ")\n",
    "axes[1].axhline(\n",
    "    y=baseline_results[\"lr_l2\"][\"test_roc_auc\"],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Baseline LR (AUC={baseline_results['lr_l2']['test_roc_auc']:.3f})\",\n",
    ")\n",
    "axes[1].plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Random\")\n",
    "axes[1].set_xlabel(\"False Positive Rate\", fontsize=11)\n",
    "axes[1].set_ylabel(\"True Positive Rate\", fontsize=11)\n",
    "axes[1].set_title(\"ROC Curve Comparison\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].legend(loc=\"lower right\", fontsize=8)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(gb_dir / \"performance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to {gb_dir / 'performance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 14. Comprehensive Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "comparison_data = {\n",
    "    \"Model\": [\n",
    "        \"Logistic Reg (L2)\",\n",
    "        \"Decision Tree\",\n",
    "        \"Random Forest\",\n",
    "        \"Gradient Boosting\",\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        baseline_results[\"lr_l2\"][\"test_accuracy\"],\n",
    "        dt_results[\"dt_tuned\"][\"test_accuracy\"],\n",
    "        rf_results[\"rf_tuned\"][\"test_accuracy\"],\n",
    "        test_acc_tuned,\n",
    "    ],\n",
    "    \"F1-Score\": [\n",
    "        baseline_results[\"lr_l2\"][\"test_f1\"],\n",
    "        dt_results[\"dt_tuned\"][\"test_f1\"],\n",
    "        rf_results[\"rf_tuned\"][\"test_f1\"],\n",
    "        test_f1_tuned,\n",
    "    ],\n",
    "    \"ROC-AUC\": [\n",
    "        baseline_results[\"lr_l2\"][\"test_roc_auc\"],\n",
    "        dt_results[\"dt_tuned\"][\"test_roc_auc\"],\n",
    "        rf_results[\"rf_tuned\"][\"test_roc_auc\"],\n",
    "        test_auc_tuned,\n",
    "    ],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best model for each metric\n",
    "print(\"\\nBest models by metric:\")\n",
    "for metric in [\"Accuracy\", \"F1-Score\", \"ROC-AUC\"]:\n",
    "    best_idx = comparison_df[metric].idxmax()\n",
    "    best_model = comparison_df.loc[best_idx, \"Model\"]\n",
    "    best_score = comparison_df.loc[best_idx, metric]\n",
    "    print(f\"  {metric:12s}: {best_model:25s} ({best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = [\"Accuracy\", \"F1-Score\", \"ROC-AUC\"]\n",
    "colors = [\"steelblue\", \"orange\", \"green\", \"purple\"]\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].bar(comparison_df[\"Model\"], comparison_df[metric], color=colors)\n",
    "    axes[i].set_ylabel(metric, fontsize=11)\n",
    "    axes[i].set_title(\n",
    "        f\"{metric} Comparison - All Models\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[i].set_ylim(\n",
    "        [min(comparison_df[metric]) * 0.95, max(comparison_df[metric]) * 1.05]\n",
    "    )\n",
    "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[i].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Annotate bars\n",
    "    for j, v in enumerate(comparison_df[metric]):\n",
    "        axes[i].text(\n",
    "            j,\n",
    "            v + 0.002,\n",
    "            f\"{v:.4f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(gb_dir / \"model_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to {gb_dir / 'model_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 15. Save Models and Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "with open(MODELS_DIR / \"gradient_boosting_tuned.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gb_tuned, f)\n",
    "with open(MODELS_DIR / \"gradient_boosting_baseline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gb_baseline, f)\n",
    "\n",
    "print(\"✓ Models saved:\")\n",
    "print(f\"  {MODELS_DIR / 'gradient_boosting_tuned.pkl'}\")\n",
    "print(f\"  {MODELS_DIR / 'gradient_boosting_baseline.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    \"gb_baseline\": {\n",
    "        \"train_accuracy\": float(accuracy_score(y_train, y_train_pred_base)),\n",
    "        \"test_accuracy\": float(accuracy_score(y_test, y_test_pred_base)),\n",
    "        \"test_precision\": float(precision_score(y_test, y_test_pred_base)),\n",
    "        \"test_recall\": float(recall_score(y_test, y_test_pred_base)),\n",
    "        \"test_f1\": float(f1_score(y_test, y_test_pred_base)),\n",
    "        \"test_roc_auc\": float(roc_auc_score(y_test, y_test_proba_base)),\n",
    "        \"n_estimators\": int(gb_baseline.n_estimators),\n",
    "        \"learning_rate\": float(gb_baseline.learning_rate),\n",
    "    },\n",
    "    \"gb_tuned\": {\n",
    "        \"train_accuracy\": float(accuracy_score(y_train, y_train_pred_tuned)),\n",
    "        \"test_accuracy\": float(test_acc_tuned),\n",
    "        \"test_precision\": float(precision_score(y_test, y_test_pred_tuned)),\n",
    "        \"test_recall\": float(recall_score(y_test, y_test_pred_tuned)),\n",
    "        \"test_f1\": float(test_f1_tuned),\n",
    "        \"test_roc_auc\": float(test_auc_tuned),\n",
    "        \"train_test_gap\": float(train_test_gap),\n",
    "        \"best_params\": random_search.best_params_,\n",
    "        \"cv_f1_score\": float(random_search.best_score_),\n",
    "    },\n",
    "    \"improvement_over_baseline\": {\n",
    "        \"accuracy\": float(acc_improvement_base),\n",
    "        \"f1\": float(f1_improvement_base),\n",
    "        \"roc_auc\": float(auc_improvement_base),\n",
    "        \"accuracy_pct\": float(\n",
    "            100 * acc_improvement_base / baseline_results[\"lr_l2\"][\"test_accuracy\"]\n",
    "        ),\n",
    "        \"f1_pct\": float(\n",
    "            100 * f1_improvement_base / baseline_results[\"lr_l2\"][\"test_f1\"]\n",
    "        ),\n",
    "        \"roc_auc_pct\": float(\n",
    "            100 * auc_improvement_base / baseline_results[\"lr_l2\"][\"test_roc_auc\"]\n",
    "        ),\n",
    "    },\n",
    "    \"improvement_over_decision_tree\": {\n",
    "        \"accuracy\": float(acc_improvement_dt),\n",
    "        \"f1\": float(f1_improvement_dt),\n",
    "        \"roc_auc\": float(auc_improvement_dt),\n",
    "        \"accuracy_pct\": float(\n",
    "            100 * acc_improvement_dt / dt_results[\"dt_tuned\"][\"test_accuracy\"]\n",
    "        ),\n",
    "        \"f1_pct\": float(100 * f1_improvement_dt / dt_results[\"dt_tuned\"][\"test_f1\"]),\n",
    "        \"roc_auc_pct\": float(\n",
    "            100 * auc_improvement_dt / dt_results[\"dt_tuned\"][\"test_roc_auc\"]\n",
    "        ),\n",
    "    },\n",
    "    \"comparison_to_random_forest\": {\n",
    "        \"accuracy\": float(acc_improvement_rf),\n",
    "        \"f1\": float(f1_improvement_rf),\n",
    "        \"roc_auc\": float(auc_improvement_rf),\n",
    "        \"accuracy_pct\": float(\n",
    "            100 * acc_improvement_rf / rf_results[\"rf_tuned\"][\"test_accuracy\"]\n",
    "        ),\n",
    "        \"f1_pct\": float(100 * f1_improvement_rf / rf_results[\"rf_tuned\"][\"test_f1\"]),\n",
    "        \"roc_auc_pct\": float(\n",
    "            100 * auc_improvement_rf / rf_results[\"rf_tuned\"][\"test_roc_auc\"]\n",
    "        ),\n",
    "    },\n",
    "    \"learning_rate_analysis\": lr_df.to_dict(\"records\"),\n",
    "}\n",
    "\n",
    "with open(MODELS_DIR / \"gradient_boosting_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {MODELS_DIR / 'gradient_boosting_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tables\n",
    "feature_importance.to_csv(TABLES_DIR / \"gb_feature_importance.csv\", index=False)\n",
    "lr_df.to_csv(TABLES_DIR / \"gb_learning_rate_analysis.csv\", index=False)\n",
    "learning_curves.to_csv(TABLES_DIR / \"gb_learning_curves.csv\", index=False)\n",
    "comparison_df.to_csv(TABLES_DIR / \"all_models_comparison.csv\", index=False)\n",
    "\n",
    "print(\"✓ Tables saved:\")\n",
    "print(f\"  {TABLES_DIR / 'gb_feature_importance.csv'}\")\n",
    "print(f\"  {TABLES_DIR / 'gb_learning_rate_analysis.csv'}\")\n",
    "print(f\"  {TABLES_DIR / 'gb_learning_curves.csv'}\")\n",
    "print(f\"  {TABLES_DIR / 'all_models_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## 16. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GRADIENT BOOSTING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Train samples: {len(X_train):,}\")\n",
    "print(f\"  Test samples: {len(X_test):,}\")\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\nBaseline Gradient Boosting (100 estimators):\")\n",
    "print(f\"  Test Accuracy: {results['gb_baseline']['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1-score: {results['gb_baseline']['test_f1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC:  {results['gb_baseline']['test_roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nTuned Gradient Boosting:\")\n",
    "print(f\"  Test Accuracy: {results['gb_tuned']['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1-score: {results['gb_tuned']['test_f1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC:  {results['gb_tuned']['test_roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for param, value in results[\"gb_tuned\"][\"best_params\"].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nImprovement over Baseline (Logistic Regression L2):\")\n",
    "print(\n",
    "    f\"  Accuracy:  {results['improvement_over_baseline']['accuracy']:+.4f} ({results['improvement_over_baseline']['accuracy_pct']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1-score:  {results['improvement_over_baseline']['f1']:+.4f} ({results['improvement_over_baseline']['f1_pct']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  ROC-AUC:   {results['improvement_over_baseline']['roc_auc']:+.4f} ({results['improvement_over_baseline']['roc_auc_pct']:+.1f}%)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nImprovement over Decision Tree:\")\n",
    "print(\n",
    "    f\"  Accuracy:  {results['improvement_over_decision_tree']['accuracy']:+.4f} ({results['improvement_over_decision_tree']['accuracy_pct']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1-score:  {results['improvement_over_decision_tree']['f1']:+.4f} ({results['improvement_over_decision_tree']['f1_pct']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  ROC-AUC:   {results['improvement_over_decision_tree']['roc_auc']:+.4f} ({results['improvement_over_decision_tree']['roc_auc_pct']:+.1f}%)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nComparison to Random Forest (Boosting vs. Bagging):\")\n",
    "print(\n",
    "    f\"  Accuracy:  {results['comparison_to_random_forest']['accuracy']:+.4f} ({results['comparison_to_random_forest']['accuracy_pct']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1-score:  {results['comparison_to_random_forest']['f1']:+.4f} ({results['comparison_to_random_forest']['f1_pct']:+.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  ROC-AUC:   {results['comparison_to_random_forest']['roc_auc']:+.4f} ({results['comparison_to_random_forest']['roc_auc_pct']:+.1f}%)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "best_model = comparison_df.loc[comparison_df[\"F1-Score\"].idxmax(), \"Model\"]\n",
    "print(f\"  - Best overall model: {best_model}\")\n",
    "print(\n",
    "    f\"  - Sequential boosting {'outperforms' if test_f1_tuned > rf_results['rf_tuned']['test_f1'] else 'underperforms'} parallel bagging\"\n",
    ")\n",
    "print(\n",
    "    f\"  - Learning curves show {'convergence' if learning_curves['test_acc'].iloc[-10:].std() < 0.001 else 'ongoing improvement'}\"\n",
    ")\n",
    "print(\n",
    "    f\"  - Top 10 features account for {100*feature_importance.head(10)['importance'].sum():.1f}% of importance\"\n",
    ")\n",
    "print(\n",
    "    f\"  - Train-test gap: {results['gb_tuned']['train_test_gap']:.4f} ({'well-regularized' if results['gb_tuned']['train_test_gap'] < 0.05 else 'some overfitting'})\"\n",
    ")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  - Notebook 55: Regime-conditional models (train separate models per regime)\")\n",
    "print(f\"  - Notebook 60: Feature importance analysis (permutation, SHAP)\")\n",
    "print(f\"  - Notebook 65: Model interpretability (decision paths, PDPs)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orderbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
